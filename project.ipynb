{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import total_ordering, reduce\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import math\n",
    "import pandas as pd\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"wiki_movie.csv\")\n",
    "docs = df[\"Plot\"]\n",
    "docs = docs[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"hello hello my name is giovanni\",\n",
    "    \"fuck this shit I am going to the beach\",\n",
    "    \"the movie last night was easily my favourite\",\n",
    "    \"the film was a complete shit\",\n",
    "    \"can you please go fuck yourself for fuck sake?\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 docs\n"
     ]
    }
   ],
   "source": [
    "vocab, postings = create_vocab_and_postings_list(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "['hello', 'my', 'name', 'is', 'giovanni', 'fuck', 'this', 'shit', 'i', 'am', 'going', 'to', 'the', 'beach', 'movie', 'last', 'night', 'was', 'easily', 'favourite', 'film', 'a', 'complete', 'can', 'you', 'please', 'go', 'yourself', 'for', 'sake']\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 0.6989700043360187,\n",
       " 'my': 0.3979400086720376,\n",
       " 'name': 0.6989700043360187,\n",
       " 'is': 0.6989700043360187,\n",
       " 'giovanni': 0.6989700043360187,\n",
       " 'fuck': 0.3979400086720376,\n",
       " 'this': 0.6989700043360187,\n",
       " 'shit': 0.3979400086720376,\n",
       " 'i': 0.6989700043360187,\n",
       " 'am': 0.6989700043360187,\n",
       " 'going': 0.6989700043360187,\n",
       " 'to': 0.6989700043360187,\n",
       " 'the': 0.22184874961635637,\n",
       " 'beach': 0.6989700043360187,\n",
       " 'movie': 0.6989700043360187,\n",
       " 'last': 0.6989700043360187,\n",
       " 'night': 0.6989700043360187,\n",
       " 'was': 0.3979400086720376,\n",
       " 'easily': 0.6989700043360187,\n",
       " 'favourite': 0.6989700043360187,\n",
       " 'film': 0.6989700043360187,\n",
       " 'a': 0.6989700043360187,\n",
       " 'complete': 0.6989700043360187,\n",
       " 'can': 0.6989700043360187,\n",
       " 'you': 0.6989700043360187,\n",
       " 'please': 0.6989700043360187,\n",
       " 'go': 0.6989700043360187,\n",
       " 'yourself': 0.6989700043360187,\n",
       " 'for': 0.6989700043360187,\n",
       " 'sake': 0.6989700043360187}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_idf = {}\n",
    "N = len(docs)\n",
    "\n",
    "for v in vocab:\n",
    "    vocab_idf[v] = math.log(N / len(postings[v]), 10)\n",
    "    \n",
    "vocab_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck shit pizza\n"
     ]
    }
   ],
   "source": [
    "myquery = \"fuck shit pizza\"\n",
    "q = preprocess_query(myquery)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.21914897\n",
      " 0.38492877 0.21914897 0.38492877 0.38492877 0.38492877 0.38492877\n",
      " 0.12217401 0.38492877 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "vectorized_docs = [vectorize(i, docs, postings, vocab, vocab_idf) for i in range(len(docs))]\n",
    "print(vectorized_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.70710678\n",
      " 0.         0.70710678 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "vectorized_query = vectorize_query(q, vocab)\n",
    "print(vectorized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.30992345 0.         0.20791533 0.27952855]\n"
     ]
    }
   ],
   "source": [
    "dot_product_scores = DotProductScore(vectorized_docs, vectorized_query)\n",
    "print(dot_product_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CosineScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.02094421 0.         0.01421214 0.01730174]\n"
     ]
    }
   ],
   "source": [
    "scores = CosineScore(q, docs, vocab_idf, postings)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract K top doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "\n",
    "def extract_top_docs(docs, scores, k):\n",
    "    docs = np.asarray(docs)\n",
    "    #ind = np.argpartition(scores, -k)[-k:]\n",
    "    ind = scores.argsort()[::-1][:k]\n",
    "    #return ind[np.argsort(docs[ind])]\n",
    "    return docs[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fuck this shit I am going to the beach',\n",
       "       'can you please go fuck yourself for fuck sake?'], dtype='<U46')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_top_docs(docs, dot_product_scores, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
