{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from vocabulary_and_postings import *\n",
    "from vectorization import *\n",
    "from scoring import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_LEMMATIZE = True\n",
    "DO_REMOVE_SW = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Processing DOCUMENTS\n",
    "doc_set = {}\n",
    "doc_id = \"\"\n",
    "doc_text = \"\"\n",
    "with open('CISI.ALL') as f:\n",
    "    lines = \"\"\n",
    "    for l in f.readlines():\n",
    "        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "doc_count = 0\n",
    "for l in lines:\n",
    "    if l.startswith(\".I\"):\n",
    "        doc_id = int(l.split(\" \")[1].strip())-1\n",
    "    elif l.startswith(\".X\"):\n",
    "        doc_set[doc_id] = doc_text.lstrip(\" \")\n",
    "        doc_id = \"\"\n",
    "        doc_text = \"\"\n",
    "    else:\n",
    "        doc_text += l.strip()[3:] + \" \" # The first 3 characters of a line can be ignored.    \n",
    "\n",
    "        \n",
    "### Processing QUERIES\n",
    "with open('CISI.QRY') as f:\n",
    "    lines = \"\"\n",
    "    for l in f.readlines():\n",
    "        lines += \"\\n\" + l.strip() if l.startswith(\".\") else \" \" + l.strip()\n",
    "    lines = lines.lstrip(\"\\n\").split(\"\\n\")\n",
    "    \n",
    "qry_set = {}\n",
    "qry_id = \"\"\n",
    "for l in lines:\n",
    "    if l.startswith(\".I\"):\n",
    "        qry_id = int(l.split(\" \")[1].strip()) -1\n",
    "    elif l.startswith(\".W\"):\n",
    "        qry_set[qry_id] = l.strip()[3:]\n",
    "        qry_id = \"\"\n",
    "\n",
    "### Processing QRELS\n",
    "rel_set = {}\n",
    "with open('CISI.REL') as f:\n",
    "    for l in f.readlines():\n",
    "        qry_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[0]) -1\n",
    "        doc_id = int(l.lstrip(\" \").strip(\"\\n\").split(\"\\t\")[0].split(\" \")[-1])-1\n",
    "        if qry_id in rel_set:\n",
    "            rel_set[qry_id].append(doc_id)\n",
    "        else:\n",
    "            rel_set[qry_id] = []\n",
    "            rel_set[qry_id].append(doc_id)\n",
    "            \n",
    "### Prepocess DOCS\n",
    "processed_docs_set = {}\n",
    "for docId, doc in doc_set.items():\n",
    "    processed_docs_set[docId] = preprocess_string(doc, remove_stop_words=DO_REMOVE_SW, lemmatize=DO_LEMMATIZE)\n",
    "    \n",
    "### Preprocess QUERY\n",
    "processed_qry_set = {}\n",
    "for qryId, qry in qry_set.items():\n",
    "    processed_qry_set[qryId] = preprocess_string(qry, remove_stop_words=DO_REMOVE_SW, lemmatize=DO_LEMMATIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOCAB AND POSTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocab and postings\n",
    "v, p = build_vocab_and_postings(processed_docs_set)\n",
    "# create vocab with idf\n",
    "v_idf = build_vocab_with_idf(v, p, processed_docs_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6773\n"
     ]
    }
   ],
   "source": [
    "print(len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VECTORIZE DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize_docs\n",
    "vector_docs = {docID : vectorize_doc(docID, processed_docs_set, p, v_idf) for docID in processed_docs_set.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VECTORIZE QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_queries = {qId : vectorize_query(q, v) for qId, q in processed_qry_set.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2620603235011236\n"
     ]
    }
   ],
   "source": [
    "MAP0 = MAP(vector_queries, vector_docs, rel_set)\n",
    "print(MAP0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20627801438639806\n"
     ]
    }
   ],
   "source": [
    "MRP = Mean_R_precision(vector_queries, vector_docs, rel_set)\n",
    "print(MRP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROPOSE DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_docs(query, k, vector_docs, v):\n",
    "    query = preprocess_string(query, remove_stop_words=DO_REMOVE_SW, lemmatize=DO_LEMMATIZE)\n",
    "    vq = vectorize_query(query, v)\n",
    "    scores = cosine_sim_score(vector_docs.values(), vq)\n",
    "    ind = extract_top_docs_index(scores, k)\n",
    "    return ind\n",
    "\n",
    "def propose_docs_with_vector_query(vq, k, vector_docs, v):\n",
    "    scores = cosine_sim_score(vector_docs.values(), vq)\n",
    "    ind = extract_top_docs_index(scores, k)\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query = What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\n",
      "\n",
      "Proposed documents indices = [ 428  446  314  721  448  602  564  174 1280 1418  588]\n",
      "\n",
      "Relevant docs indices = [27, 34, 37, 41, 42, 51, 64, 75, 85, 149, 188, 191, 192, 194, 214, 268, 290, 319, 428, 464, 465, 481, 482, 509, 523, 540, 575, 581, 588, 602, 649, 679, 710, 721, 725, 782, 812, 819, 867, 868, 893, 1161, 1163, 1194, 1195, 1280]\n",
      "\n",
      "Precision at k10 = 0.5\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "query = qry_set[idx]\n",
    "\n",
    "print(f'Query = {query}\\n')\n",
    "\n",
    "proposed_idx = propose_docs(query, k, vector_docs, v)\n",
    "print(f\"Proposed documents indices = {proposed_idx}\\n\")\n",
    "\n",
    "print(f'Relevant docs indices = {rel_set[idx]}\\n')\n",
    "\n",
    "print(f\"Precision at k10 = {len(set(rel_set[idx]).intersection(set(proposed_idx))) / 10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_docs_with_pseudo_relevance(query, k, vector_docs, postings, v, v_idf, processed_docs_set):\n",
    "    \n",
    "    TOP_K = 20\n",
    "    TOP_TERMS = 30\n",
    "    \n",
    "    ind = propose_docs(query, TOP_K, vector_docs, v)\n",
    "    \n",
    "    top_docs = {i : processed_docs_set[i] for i in ind}\n",
    "    \n",
    "    terms_tf_idf = [0] * len(v)\n",
    "    \n",
    "    for docId, doc_text in top_docs.items():\n",
    "        for tk in doc_text:\n",
    "            new_tf = postings[tk][docId]\n",
    "            tokenId, new_idf = v_idf[tk]\n",
    "            new_tf_idf = new_tf * new_idf\n",
    "            \n",
    "            if terms_tf_idf[tokenId] < new_tf_idf:\n",
    "                terms_tf_idf[tokenId] = new_tf_idf\n",
    "            \n",
    "          # if tokenId in terms_tf_idf:\n",
    "          #     if new_tf_idf > terms_tf_idf[tk]:\n",
    "          #         terms_tf_idf[tk] = new_tf_idf\n",
    "          # else:\n",
    "          #     terms_tf_idf[tk] = new_tf_idf\n",
    "          #     \n",
    "            \n",
    "    terms_tf_idf = np.asarray(terms_tf_idf)\n",
    "    ind_best_tokens = terms_tf_idf.argsort()[::-1][:TOP_TERMS]\n",
    "    \n",
    "    tokens = [v[i] for i in ind_best_tokens]\n",
    "    \n",
    "    extra_terms = ' '.join(tokens)\n",
    "    new_query = query + ' ' +extra_terms\n",
    "    \n",
    "    new_ind = propose_docs(new_query, k, vector_docs, v)\n",
    "    \n",
    "    return new_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_docs_with_feedback(query, k, vector_docs, v, alpha = 1, beta = 0.75, gamma = 0.15):\n",
    "    # proposal docs\n",
    "    ind = propose_docs(query, k, vector_docs, v)\n",
    "    top_docs = {i : processed_docs_set[i] for i in ind}\n",
    "    \n",
    "    # ask user for feedback:\n",
    "    print(\"Insert feedback for every proposed document (1 = relevant, 0 = non relevant):\\n\")\n",
    "    r_doc_idx = []\n",
    "    nr_doc_idx = []\n",
    "    for docId in top_docs.keys():\n",
    "        is_relevant = int(input(f'Is document {docId} relevant?'))\n",
    "        print(is_relevant)\n",
    "        if is_relevant == 1:\n",
    "            r_doc_idx.append(docId)\n",
    "        elif is_relevant == 0:\n",
    "            nr_doc_idx.append(docId)\n",
    "        else:\n",
    "            raise Exception\n",
    "            \n",
    "    # modify query\n",
    "    query = preprocess_string(query, remove_stop_words=DO_REMOVE_SW, lemmatize=DO_LEMMATIZE)\n",
    "    vq = vectorize_query(query, v)  \n",
    "    \n",
    "    rd = np.array([vector_docs[i] for i in r_doc_idx])\n",
    "    nrd = np.array([vector_docs[i] for i in nr_doc_idx])\n",
    "    \n",
    "    centroid_r = np.zeros_like(vq)\n",
    "    centroid_nr = np.zeros_like(vq)\n",
    "    \n",
    "    for i in r_doc_idx:\n",
    "        centroid_r += vector_docs[i]\n",
    "    for i in nr_doc_idx:\n",
    "        centroid_nr += vector_docs[i]\n",
    "        \n",
    "    centroid_r = centroid_r / len(r_doc_idx)\n",
    "    centroid_nr = centroid_nr / len(nr_doc_idx)\n",
    "    \n",
    "    new_query = alpha * vq + beta * centroid_r - gamma * centroid_nr\n",
    "    new_query = new_query / np.linalg.norm(new_query)\n",
    "    \n",
    "    ind = propose_docs_with_vector_query(new_query, k, vector_docs, v)\n",
    "    \n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant docs indices = [28, 67, 196, 212, 213, 308, 318, 323, 428, 498, 635, 668, 669, 673, 689, 691, 694, 699, 703, 708, 719, 730, 732, 737, 739, 1135]\n",
      "\n",
      "Proposed documents indices = [1154 1137 1135  564  174 1157  531  308  789 1395]\n",
      "Precision at k10 = 0.2\n",
      "\n",
      "Proposed documents indices psuedo_rel = [ 610  561 1326  487 1395  564  308  445  658 1137]\n",
      "Precision at k10 = 0.1\n",
      "Insert feedback for every proposed document (1 = relevant, 0 = non relevant):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is document 1154 relevant? 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is document 1137 relevant? 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is document 1135 relevant? 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is document 564 relevant? 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is document 174 relevant? 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is document 1157 relevant? 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is document 531 relevant? 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is document 308 relevant? 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is document 789 relevant? 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is document 1395 relevant? 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Proposed documents indices feedback = [1135  308  564 1326  174  482 1154  178   71  450]\n",
      "Precision at k10 = 0.2\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "idx = 1\n",
    "\n",
    "query = qry_set[idx]\n",
    "\n",
    "print(f'Relevant docs indices = {rel_set[idx]}\\n')\n",
    "\n",
    "proposed_idx = propose_docs(query, k, vector_docs, v)\n",
    "print(f\"Proposed documents indices = {proposed_idx}\")\n",
    "print(f\"Precision at k{k} = {len(set(rel_set[idx]).intersection(set(proposed_idx))) / k}\\n\")\n",
    "\n",
    "ind_pseduo_rel = propose_docs_with_pseudo_relevance(query, k, vector_docs, p, v, v_idf, processed_docs_set)\n",
    "print(f\"Proposed documents indices psuedo_rel = {ind_pseduo_rel}\")\n",
    "print(f\"Precision at k{k} = {len(set(rel_set[idx]).intersection(set(ind_pseduo_rel))) / k}\")\n",
    "\n",
    "ind_feedback = propose_docs_with_feedback(query, k, vector_docs, v)\n",
    "print(f\"Proposed documents indices feedback = {ind_feedback}\")\n",
    "print(f\"Precision at k{k} = {len(set(rel_set[idx]).intersection(set(ind_feedback))) / k}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
