{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from vocabulary_and_postings import *\n",
    "from vectorization import *\n",
    "from scoring import *\n",
    "from vect_answer import *\n",
    "from fast_answer import *\n",
    "from evaluation import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_set, qry_set, rel_set = load_dataset(\"CISI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASET INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # N Documents = 1460\n",
      " # N Queries = 112\n",
      " # Avg number of relevant docs per query = 40.97\n",
      " # Min number of relevant docs per query = 1\n",
      " # Max number of relevant docs per query = 155\n",
      " # Query without relevant docs = [35, 37, 39, 46, 47, 50, 52, 58, 59, 62, 63, 67, 69, 71, 72, 73, 74, 76, 77, 79, 82, 84, 85, 86, 87, 88, 90, 92, 93, 102, 104, 105, 106, 107, 109, 111]\n"
     ]
    }
   ],
   "source": [
    "print(f' # N Documents = {len(doc_set)}\\n # N Queries = {len(qry_set)}')\n",
    "print(f' # Avg number of relevant docs per query = {np.mean([len(r) for r in rel_set.values()]):.2f}' )\n",
    "print(f' # Min number of relevant docs per query = {int(np.min([len(r) for r in rel_set.values()]))}' )\n",
    "print(f' # Max number of relevant docs per query = {int(np.max([len(r) for r in rel_set.values()]))}' )\n",
    "print(f' # Query without relevant docs = {[i for i in qry_set.keys() if i not in rel_set.keys()]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOPWORDS NOT REMOVED, NO LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords = False, lemmatize = False\n",
      "Vocabulary contains 10759 tokens\n",
      "\n",
      "Standard vect | MAP = 0.21 \t MRP = 0.16 \t MK10 = 0.16\n",
      "Standard fast | MAP = 0.21 \t MRP = 0.16 \t MK10 = 0.16\n",
      "\n",
      "\n",
      "Pseu exp vect | MAP = 0.19 \t MRP = 0.15 \t MK10 = 0.15\n",
      "Pseu exp fast | MAP = 0.20 \t MRP = 0.17 \t MK10 = 0.17\n",
      "\n",
      "\n",
      "Pseu mov vect | MAP = 0.21 \t MRP = 0.17 \t MK10 = 0.16\n",
      "Pseu mov fast | MAP = 0.18 \t MRP = 0.16 \t MK10 = 0.14\n",
      "\n",
      "\n",
      "Feed mov vect | MAP = 0.43 \t MRP = 0.25 \t MK10 = 0.25\n",
      "Feed mov fast | MAP = 0.38 \t MRP = 0.21 \t MK10 = 0.21\n",
      "\n",
      "\n",
      "Feed exp vect | MAP = 0.38 \t MRP = 0.21 \t MK10 = 0.20\n",
      "Feed exp fast | MAP = 0.36 \t MRP = 0.20 \t MK10 = 0.20\n"
     ]
    }
   ],
   "source": [
    "DO_LEMMATIZE = False\n",
    "DO_REMOVE_SW = False\n",
    "\n",
    "print_eval(doc_set=doc_set, qry_set=qry_set, rel_set=rel_set, remove_sw=DO_REMOVE_SW, lemmatize=DO_LEMMATIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOPWORDS REMOVED, NO LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords = True, lemmatize = False\n",
      "Vocabulary contains 10639 tokens\n",
      "\n",
      "Standard vect | MAP = 0.23 \t MRP = 0.18 \t MK10 = 0.18\n",
      "Standard fast | MAP = 0.25 \t MRP = 0.19 \t MK10 = 0.19\n",
      "\n",
      "\n",
      "Pseu exp vect | MAP = 0.22 \t MRP = 0.18 \t MK10 = 0.17\n",
      "Pseu exp fast | MAP = 0.14 \t MRP = 0.10 \t MK10 = 0.10\n",
      "\n",
      "\n",
      "Pseu mov vect | MAP = 0.22 \t MRP = 0.19 \t MK10 = 0.18\n",
      "Pseu mov fast | MAP = 0.23 \t MRP = 0.19 \t MK10 = 0.18\n",
      "\n",
      "\n",
      "Feed mov vect | MAP = 0.45 \t MRP = 0.27 \t MK10 = 0.26\n",
      "Feed mov fast | MAP = 0.43 \t MRP = 0.24 \t MK10 = 0.24\n",
      "\n",
      "\n",
      "Feed exp vect | MAP = 0.40 \t MRP = 0.23 \t MK10 = 0.22\n",
      "Feed exp fast | MAP = 0.41 \t MRP = 0.24 \t MK10 = 0.23\n"
     ]
    }
   ],
   "source": [
    "DO_LEMMATIZE = False\n",
    "DO_REMOVE_SW = True\n",
    "\n",
    "print_eval(doc_set=doc_set, qry_set=qry_set, rel_set=rel_set, remove_sw=DO_REMOVE_SW, lemmatize=DO_LEMMATIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOPWORDS REMOVED AND LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords = True, lemmatize = True\n",
      "Vocabulary contains 6773 tokens\n",
      "\n",
      "Standard vect | MAP = 0.26 \t MRP = 0.21 \t MK10 = 0.20\n",
      "Standard fast | MAP = 0.28 \t MRP = 0.21 \t MK10 = 0.20\n",
      "\n",
      "\n",
      "Pseu exp vect | MAP = 0.23 \t MRP = 0.18 \t MK10 = 0.18\n",
      "Pseu exp fast | MAP = 0.24 \t MRP = 0.18 \t MK10 = 0.17\n",
      "\n",
      "\n",
      "Pseu mov vect | MAP = 0.26 \t MRP = 0.22 \t MK10 = 0.21\n",
      "Pseu mov fast | MAP = 0.24 \t MRP = 0.20 \t MK10 = 0.18\n",
      "\n",
      "\n",
      "Feed mov vect | MAP = 0.48 \t MRP = 0.29 \t MK10 = 0.29\n",
      "Feed mov fast | MAP = 0.42 \t MRP = 0.25 \t MK10 = 0.25\n",
      "\n",
      "\n",
      "Feed exp vect | MAP = 0.42 \t MRP = 0.24 \t MK10 = 0.24\n",
      "Feed exp fast | MAP = 0.40 \t MRP = 0.24 \t MK10 = 0.23\n"
     ]
    }
   ],
   "source": [
    "DO_LEMMATIZE = True\n",
    "DO_REMOVE_SW = True\n",
    "\n",
    "print_eval(doc_set=doc_set, qry_set=qry_set, rel_set=rel_set, remove_sw=DO_REMOVE_SW, lemmatize=DO_LEMMATIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
